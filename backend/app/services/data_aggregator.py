"""
CWatcher æ™‚åºæ•¸æ“šèšåˆç³»çµ±

å°ˆé–€è² è²¬ç›£æ§æ•¸æ“šçš„çµ±è¨ˆèšåˆã€åœ–è¡¨æ•¸æ“šç”Ÿæˆå’Œæ­·å²æ•¸æ“šæŸ¥è©¢
æ”¯æ´å¤šæ™‚é–“ç¯„åœçš„æ•¸æ“šèšåˆå’Œé«˜æ•ˆçš„æ™‚åºæ•¸æ“šæŸ¥è©¢
"""

import asyncio
import logging
import time
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from sqlalchemy.orm import Session
from sqlalchemy import text, func, desc, asc, and_, or_
from statistics import mean, median
import math

from core.deps import get_db
from models.system_metrics import SystemMetrics
from models.server import Server
from core.config import settings

# è¨­å®šæ—¥èªŒ
logger = logging.getLogger(__name__)


class TimeRange(Enum):
    """æ™‚é–“ç¯„åœæšèˆ‰"""
    HOUR_1 = "1h"
    HOUR_6 = "6h" 
    HOUR_24 = "24h"
    DAY_7 = "7d"
    DAY_30 = "30d"


class AggregationType(Enum):
    """èšåˆé¡å‹æšèˆ‰"""
    AVERAGE = "avg"
    MAXIMUM = "max"
    MINIMUM = "min"
    SUM = "sum"
    COUNT = "count"
    PERCENTILE_95 = "p95"


@dataclass
class TimeSeriesPoint:
    """æ™‚åºæ•¸æ“šé»"""
    timestamp: datetime
    value: Optional[float]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "timestamp": self.timestamp.isoformat(),
            "value": self.value
        }


@dataclass
class MetricSummary:
    """æŒ‡æ¨™æ‘˜è¦çµ±è¨ˆ"""
    current_value: Optional[float] = None
    average_value: Optional[float] = None
    maximum_value: Optional[float] = None
    minimum_value: Optional[float] = None
    percentile_95: Optional[float] = None
    total_samples: int = 0
    trend_direction: str = "stable"  # up, down, stable
    trend_percentage: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "current": self.current_value,
            "average": self.average_value,
            "maximum": self.maximum_value,
            "minimum": self.minimum_value,
            "p95": self.percentile_95,
            "samples": self.total_samples,
            "trend": {
                "direction": self.trend_direction,
                "percentage": self.trend_percentage
            }
        }


@dataclass
class ChartData:
    """åœ–è¡¨æ•¸æ“š"""
    metric_name: str
    time_range: TimeRange
    time_series: List[TimeSeriesPoint] = field(default_factory=list)
    summary: Optional[MetricSummary] = None
    unit: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "metric": self.metric_name,
            "time_range": self.time_range.value,
            "unit": self.unit,
            "data": [point.to_dict() for point in self.time_series],
            "summary": self.summary.to_dict() if self.summary else None
        }


class TimeSeriesAggregator:
    """æ™‚åºæ•¸æ“šèšåˆå™¨"""
    
    # æ™‚é–“ç¯„åœé…ç½®
    TIME_RANGE_CONFIG = {
        TimeRange.HOUR_1: {"minutes": 60, "interval_minutes": 1, "max_points": 60},
        TimeRange.HOUR_6: {"minutes": 360, "interval_minutes": 5, "max_points": 72},
        TimeRange.HOUR_24: {"minutes": 1440, "interval_minutes": 15, "max_points": 96},
        TimeRange.DAY_7: {"minutes": 10080, "interval_minutes": 60, "max_points": 168},
        TimeRange.DAY_30: {"minutes": 43200, "interval_minutes": 240, "max_points": 180}
    }
    
    def __init__(self):
        self.db_session_factory = get_db
    
    async def get_metric_time_series(
        self,
        server_id: int,
        metric_name: str,
        time_range: TimeRange,
        aggregation: AggregationType = AggregationType.AVERAGE
    ) -> ChartData:
        """
        å–å¾—æŒ‡æ¨™çš„æ™‚åºæ•¸æ“š
        
        Args:
            server_id: ä¼ºæœå™¨ID
            metric_name: æŒ‡æ¨™åç¨± (cpu_usage_percent, memory_usage_percent, etc.)
            time_range: æ™‚é–“ç¯„åœ
            aggregation: èšåˆé¡å‹
        """
        try:
            # å–å¾—æ™‚é–“ç¯„åœé…ç½®
            config = self.TIME_RANGE_CONFIG[time_range]
            end_time = datetime.now()
            start_time = end_time - timedelta(minutes=config["minutes"])
            
            # æŸ¥è©¢åŸå§‹æ•¸æ“š
            db = next(self.db_session_factory())
            try:
                raw_data = await self._query_raw_metrics(
                    db, server_id, metric_name, start_time, end_time
                )
                
                # èšåˆæ•¸æ“š
                time_series = await self._aggregate_time_series(
                    raw_data, config["interval_minutes"], aggregation
                )
                
                # è¨ˆç®—æ‘˜è¦çµ±è¨ˆ
                summary = await self._calculate_summary_stats(raw_data, aggregation)
                
                # å–å¾—å–®ä½
                unit = self._get_metric_unit(metric_name)
                
                return ChartData(
                    metric_name=metric_name,
                    time_range=time_range,
                    time_series=time_series,
                    summary=summary,
                    unit=unit
                )
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"å–å¾—æ™‚åºæ•¸æ“šå¤±æ•—: {e}")
            return ChartData(
                metric_name=metric_name,
                time_range=time_range,
                unit=self._get_metric_unit(metric_name)
            )
    
    async def get_multiple_metrics_chart_data(
        self,
        server_id: int,
        metric_names: List[str],
        time_range: TimeRange
    ) -> Dict[str, ChartData]:
        """å–å¾—å¤šå€‹æŒ‡æ¨™çš„åœ–è¡¨æ•¸æ“š"""
        try:
            # ä¸¦è¡ŒæŸ¥è©¢æ‰€æœ‰æŒ‡æ¨™
            tasks = [
                self.get_metric_time_series(server_id, metric_name, time_range)
                for metric_name in metric_names
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµ„ç¹”çµæœ
            chart_data = {}
            for i, result in enumerate(results):
                metric_name = metric_names[i]
                if isinstance(result, ChartData):
                    chart_data[metric_name] = result
                else:
                    logger.warning(f"å–å¾— {metric_name} æ•¸æ“šå¤±æ•—: {result}")
                    chart_data[metric_name] = ChartData(
                        metric_name=metric_name,
                        time_range=time_range,
                        unit=self._get_metric_unit(metric_name)
                    )
            
            return chart_data
            
        except Exception as e:
            logger.error(f"å–å¾—å¤šå€‹æŒ‡æ¨™åœ–è¡¨æ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    async def get_server_dashboard_data(
        self,
        server_id: int,
        time_range: TimeRange = TimeRange.HOUR_1
    ) -> Dict[str, Any]:
        """
        å–å¾—ä¼ºæœå™¨å„€è¡¨æ¿æ•¸æ“š
        ç¬¦åˆ UI åŸå‹éœ€æ±‚çš„æ ¼å¼
        """
        try:
            # æ ¸å¿ƒç›£æ§æŒ‡æ¨™
            core_metrics = [
                "cpu_usage_percent",
                "memory_usage_percent", 
                "disk_usage_percent"
            ]
            
            # å–å¾—åœ–è¡¨æ•¸æ“š
            chart_data = await self.get_multiple_metrics_chart_data(
                server_id, core_metrics, time_range
            )
            
            # å–å¾—æœ€æ–°æ•¸æ“šé»
            latest_data = await self._get_latest_metrics(server_id)
            
            # çµ„ç¹”å„€è¡¨æ¿æ•¸æ“š
            dashboard_data = {
                "server_id": server_id,
                "time_range": time_range.value,
                "timestamp": datetime.now().isoformat(),
                "charts": {
                    metric: data.to_dict() 
                    for metric, data in chart_data.items()
                },
                "current_values": latest_data,
                "status": self._calculate_overall_status(latest_data)
            }
            
            return dashboard_data
            
        except Exception as e:
            logger.error(f"å–å¾—å„€è¡¨æ¿æ•¸æ“šå¤±æ•—: {e}")
            return {
                "server_id": server_id,
                "time_range": time_range.value,
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    async def _query_raw_metrics(
        self,
        db: Session,
        server_id: int,
        metric_name: str,
        start_time: datetime,
        end_time: datetime
    ) -> List[Tuple[datetime, float]]:
        """æŸ¥è©¢åŸå§‹æŒ‡æ¨™æ•¸æ“š"""
        try:
            # æ§‹å»ºæŸ¥è©¢
            query = db.query(
                SystemMetrics.timestamp,
                getattr(SystemMetrics, metric_name)
            ).filter(
                SystemMetrics.server_id == server_id,
                SystemMetrics.timestamp >= start_time,
                SystemMetrics.timestamp <= end_time,
                SystemMetrics.collection_success == True,
                getattr(SystemMetrics, metric_name).isnot(None)
            ).order_by(SystemMetrics.timestamp)
            
            results = query.all()
            
            # éæ¿¾æœ‰æ•ˆæ•¸æ“š
            return [
                (timestamp, float(value)) 
                for timestamp, value in results 
                if value is not None
            ]
            
        except Exception as e:
            logger.error(f"æŸ¥è©¢åŸå§‹æ•¸æ“šå¤±æ•—: {e}")
            return []
    
    async def _aggregate_time_series(
        self,
        raw_data: List[Tuple[datetime, float]],
        interval_minutes: int,
        aggregation: AggregationType
    ) -> List[TimeSeriesPoint]:
        """èšåˆæ™‚åºæ•¸æ“š"""
        if not raw_data:
            return []
        
        try:
            # æŒ‰æ™‚é–“é–“éš”åˆ†çµ„æ•¸æ“š
            groups = {}
            
            for timestamp, value in raw_data:
                # è¨ˆç®—æ™‚é–“é–“éš”çš„èµ·å§‹æ™‚é–“
                interval_start = self._round_to_interval(timestamp, interval_minutes)
                
                if interval_start not in groups:
                    groups[interval_start] = []
                groups[interval_start].append(value)
            
            # èšåˆæ¯å€‹æ™‚é–“é–“éš”çš„æ•¸æ“š
            time_series = []
            for interval_start in sorted(groups.keys()):
                values = groups[interval_start]
                
                # æ ¹æ“šèšåˆé¡å‹è¨ˆç®—å€¼
                if aggregation == AggregationType.AVERAGE:
                    aggregated_value = mean(values)
                elif aggregation == AggregationType.MAXIMUM:
                    aggregated_value = max(values)
                elif aggregation == AggregationType.MINIMUM:
                    aggregated_value = min(values)
                elif aggregation == AggregationType.SUM:
                    aggregated_value = sum(values)
                elif aggregation == AggregationType.COUNT:
                    aggregated_value = len(values)
                elif aggregation == AggregationType.PERCENTILE_95:
                    aggregated_value = self._percentile(values, 95)
                else:
                    aggregated_value = mean(values)
                
                time_series.append(TimeSeriesPoint(
                    timestamp=interval_start,
                    value=round(aggregated_value, 2)
                ))
            
            return time_series
            
        except Exception as e:
            logger.error(f"èšåˆæ™‚åºæ•¸æ“šå¤±æ•—: {e}")
            return []
    
    async def _calculate_summary_stats(
        self,
        raw_data: List[Tuple[datetime, float]],
        aggregation: AggregationType
    ) -> MetricSummary:
        """è¨ˆç®—æ‘˜è¦çµ±è¨ˆ"""
        if not raw_data:
            return MetricSummary()
        
        try:
            values = [value for _, value in raw_data]
            
            summary = MetricSummary(
                current_value=values[-1] if values else None,
                average_value=round(mean(values), 2),
                maximum_value=round(max(values), 2),
                minimum_value=round(min(values), 2),
                percentile_95=round(self._percentile(values, 95), 2),
                total_samples=len(values)
            )
            
            # è¨ˆç®—è¶¨å‹¢
            if len(raw_data) >= 2:
                trend = self._calculate_trend(raw_data)
                summary.trend_direction = trend["direction"]
                summary.trend_percentage = trend["percentage"]
            
            return summary
            
        except Exception as e:
            logger.error(f"è¨ˆç®—æ‘˜è¦çµ±è¨ˆå¤±æ•—: {e}")
            return MetricSummary()
    
    async def _get_latest_metrics(self, server_id: int) -> Dict[str, Any]:
        """å–å¾—æœ€æ–°çš„ç›£æ§æ•¸æ“š"""
        try:
            db = next(self.db_session_factory())
            try:
                latest = db.query(SystemMetrics).filter(
                    SystemMetrics.server_id == server_id,
                    SystemMetrics.collection_success == True
                ).order_by(desc(SystemMetrics.timestamp)).first()
                
                if not latest:
                    return {}
                
                return {
                    "timestamp": latest.timestamp.isoformat(),
                    "cpu_usage_percent": latest.cpu_usage_percent,
                    "memory_usage_percent": latest.memory_usage_percent,
                    "disk_usage_percent": latest.disk_usage_percent,
                    "load_average_1m": latest.load_average_1m,
                    "network_bytes_sent_per_sec": latest.network_bytes_sent_per_sec,
                    "network_bytes_recv_per_sec": latest.network_bytes_recv_per_sec
                }
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"å–å¾—æœ€æ–°æ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    def _round_to_interval(self, timestamp: datetime, interval_minutes: int) -> datetime:
        """å°‡æ™‚é–“æˆ³å››æ¨äº”å…¥åˆ°æŒ‡å®šé–“éš”"""
        # è¨ˆç®—å¾åˆå¤œé–‹å§‹çš„åˆ†é˜æ•¸
        minutes_since_midnight = timestamp.hour * 60 + timestamp.minute
        
        # å››æ¨äº”å…¥åˆ°é–“éš”
        rounded_minutes = (minutes_since_midnight // interval_minutes) * interval_minutes
        
        # è¿”å›å››æ¨äº”å…¥çš„æ™‚é–“æˆ³
        return timestamp.replace(
            hour=rounded_minutes // 60,
            minute=rounded_minutes % 60,
            second=0,
            microsecond=0
        )
    
    def _percentile(self, values: List[float], percentile: int) -> float:
        """è¨ˆç®—ç™¾åˆ†ä½æ•¸"""
        if not values:
            return 0.0
        
        sorted_values = sorted(values)
        k = (len(sorted_values) - 1) * percentile / 100.0
        f = math.floor(k)
        c = math.ceil(k)
        
        if f == c:
            return sorted_values[int(k)]
        
        return sorted_values[int(f)] * (c - k) + sorted_values[int(c)] * (k - f)
    
    def _calculate_trend(self, raw_data: List[Tuple[datetime, float]]) -> Dict[str, Any]:
        """è¨ˆç®—æ•¸æ“šè¶¨å‹¢"""
        if len(raw_data) < 2:
            return {"direction": "stable", "percentage": 0.0}
        
        try:
            # å–å‰25%å’Œå¾Œ25%çš„æ•¸æ“šé€²è¡Œæ¯”è¼ƒ
            quarter_size = max(1, len(raw_data) // 4)
            
            early_values = [value for _, value in raw_data[:quarter_size]]
            recent_values = [value for _, value in raw_data[-quarter_size:]]
            
            early_avg = mean(early_values)
            recent_avg = mean(recent_values)
            
            if early_avg == 0:
                return {"direction": "stable", "percentage": 0.0}
            
            change_percentage = ((recent_avg - early_avg) / early_avg) * 100
            
            if abs(change_percentage) < 5:  # 5% ä»¥å…§èªç‚ºæ˜¯ç©©å®š
                direction = "stable"
            elif change_percentage > 0:
                direction = "up"
            else:
                direction = "down"
            
            return {
                "direction": direction,
                "percentage": round(abs(change_percentage), 1)
            }
            
        except Exception as e:
            logger.warning(f"è¨ˆç®—è¶¨å‹¢å¤±æ•—: {e}")
            return {"direction": "stable", "percentage": 0.0}
    
    def _get_metric_unit(self, metric_name: str) -> str:
        """å–å¾—æŒ‡æ¨™å–®ä½"""
        unit_mapping = {
            "cpu_usage_percent": "%",
            "memory_usage_percent": "%",
            "disk_usage_percent": "%",
            "swap_usage_percent": "%",
            "cpu_user_percent": "%",
            "cpu_system_percent": "%",
            "cpu_idle_percent": "%",
            "cpu_iowait_percent": "%",
            "load_average_1m": "",
            "load_average_5m": "",
            "load_average_15m": "",
            "memory_total_mb": "MB",
            "memory_used_mb": "MB",
            "memory_available_mb": "MB",
            "memory_cached_mb": "MB",
            "disk_read_bytes_per_sec": "B/s",
            "disk_write_bytes_per_sec": "B/s",
            "network_bytes_sent_per_sec": "B/s",
            "network_bytes_recv_per_sec": "B/s",
            "cpu_frequency_mhz": "MHz",
            "uptime_seconds": "s"
        }
        return unit_mapping.get(metric_name, "")
    
    def _calculate_overall_status(self, latest_data: Dict[str, Any]) -> str:
        """è¨ˆç®—æ•´é«”ç‹€æ…‹"""
        if not latest_data:
            return "unknown"
        
        try:
            cpu_usage = latest_data.get("cpu_usage_percent", 0)
            memory_usage = latest_data.get("memory_usage_percent", 0)
            disk_usage = latest_data.get("disk_usage_percent", 0)
            
            # æª¢æŸ¥åš´é‡ç‹€æ…‹
            if cpu_usage >= 90 or memory_usage >= 95 or disk_usage >= 95:
                return "critical"
            
            # æª¢æŸ¥è­¦å‘Šç‹€æ…‹
            if cpu_usage >= 80 or memory_usage >= 85 or disk_usage >= 90:
                return "warning"
            
            return "normal"
            
        except Exception:
            return "unknown"


class BatchDataAggregator:
    """æ‰¹é‡æ•¸æ“šèšåˆå™¨"""
    
    def __init__(self):
        self.aggregator = TimeSeriesAggregator()
    
    async def generate_server_charts_batch(
        self,
        server_ids: List[int],
        metric_names: List[str],
        time_range: TimeRange
    ) -> Dict[int, Dict[str, ChartData]]:
        """æ‰¹é‡ç”Ÿæˆå¤šå°ä¼ºæœå™¨çš„åœ–è¡¨æ•¸æ“š"""
        try:
            # ç‚ºæ¯å°ä¼ºæœå™¨ä¸¦è¡Œç”Ÿæˆåœ–è¡¨æ•¸æ“š
            tasks = [
                self.aggregator.get_multiple_metrics_chart_data(
                    server_id, metric_names, time_range
                )
                for server_id in server_ids
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµ„ç¹”çµæœ
            batch_results = {}
            for i, result in enumerate(results):
                server_id = server_ids[i]
                if isinstance(result, dict):
                    batch_results[server_id] = result
                else:
                    logger.warning(f"ä¼ºæœå™¨ {server_id} æ•¸æ“šç”Ÿæˆå¤±æ•—: {result}")
                    batch_results[server_id] = {}
            
            return batch_results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç”Ÿæˆåœ–è¡¨æ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    async def generate_dashboard_data_batch(
        self,
        server_ids: List[int],
        time_range: TimeRange = TimeRange.HOUR_1
    ) -> Dict[int, Dict[str, Any]]:
        """æ‰¹é‡ç”Ÿæˆå„€è¡¨æ¿æ•¸æ“š"""
        try:
            # ä¸¦è¡Œç”Ÿæˆæ‰€æœ‰ä¼ºæœå™¨çš„å„€è¡¨æ¿æ•¸æ“š
            tasks = [
                self.aggregator.get_server_dashboard_data(server_id, time_range)
                for server_id in server_ids
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµ„ç¹”çµæœ
            batch_results = {}
            for i, result in enumerate(results):
                server_id = server_ids[i]
                if isinstance(result, dict):
                    batch_results[server_id] = result
                else:
                    logger.warning(f"ä¼ºæœå™¨ {server_id} å„€è¡¨æ¿æ•¸æ“šç”Ÿæˆå¤±æ•—: {result}")
                    batch_results[server_id] = {
                        "server_id": server_id,
                        "error": str(result)
                    }
            
            return batch_results
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç”Ÿæˆå„€è¡¨æ¿æ•¸æ“šå¤±æ•—: {e}")
            return {}


class HistoricalDataManager:
    """æ­·å²æ•¸æ“šç®¡ç†å™¨"""
    
    def __init__(self):
        self.db_session_factory = get_db
    
    async def get_historical_summary(
        self,
        server_id: int,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """å–å¾—æ­·å²æ•¸æ“šæ‘˜è¦"""
        try:
            db = next(self.db_session_factory())
            try:
                # æŸ¥è©¢æ­·å²æ•¸æ“š
                query = db.query(SystemMetrics).filter(
                    SystemMetrics.server_id == server_id,
                    SystemMetrics.timestamp >= start_date,
                    SystemMetrics.timestamp <= end_date,
                    SystemMetrics.collection_success == True
                )
                
                # çµ±è¨ˆæŸ¥è©¢
                total_records = query.count()
                
                if total_records == 0:
                    return {
                        "server_id": server_id,
                        "period": {
                            "start": start_date.isoformat(),
                            "end": end_date.isoformat()
                        },
                        "total_records": 0,
                        "message": "æ­¤æ™‚é–“ç¯„åœå…§ç„¡æ•¸æ“š"
                    }
                
                # èšåˆçµ±è¨ˆ
                stats = query.with_entities(
                    func.avg(SystemMetrics.cpu_usage_percent).label('avg_cpu'),
                    func.max(SystemMetrics.cpu_usage_percent).label('max_cpu'),
                    func.avg(SystemMetrics.memory_usage_percent).label('avg_memory'),
                    func.max(SystemMetrics.memory_usage_percent).label('max_memory'),
                    func.avg(SystemMetrics.disk_usage_percent).label('avg_disk'),
                    func.max(SystemMetrics.disk_usage_percent).label('max_disk')
                ).first()
                
                return {
                    "server_id": server_id,
                    "period": {
                        "start": start_date.isoformat(),
                        "end": end_date.isoformat()
                    },
                    "total_records": total_records,
                    "averages": {
                        "cpu_usage_percent": round(stats.avg_cpu or 0, 2),
                        "memory_usage_percent": round(stats.avg_memory or 0, 2),
                        "disk_usage_percent": round(stats.avg_disk or 0, 2)
                    },
                    "peaks": {
                        "cpu_usage_percent": round(stats.max_cpu or 0, 2),        
                        "memory_usage_percent": round(stats.max_memory or 0, 2),
                        "disk_usage_percent": round(stats.max_disk or 0, 2)
                    }
                }
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"å–å¾—æ­·å²æ•¸æ“šæ‘˜è¦å¤±æ•—: {e}")
            return {
                "server_id": server_id,
                "error": str(e)
            }
    
    async def export_historical_data(
        self,
        server_id: int,
        start_date: datetime,
        end_date: datetime,
        format: str = "json"
    ) -> Dict[str, Any]:
        """åŒ¯å‡ºæ­·å²æ•¸æ“š"""
        try:
            db = next(self.db_session_factory())
            try:
                # æŸ¥è©¢æ•¸æ“š
                records = db.query(SystemMetrics).filter(
                    SystemMetrics.server_id == server_id,
                    SystemMetrics.timestamp >= start_date,
                    SystemMetrics.timestamp <= end_date,
                    SystemMetrics.collection_success == True
                ).order_by(SystemMetrics.timestamp).all()
                
                if format.lower() == "csv":
                    # CSV æ ¼å¼
                    csv_data = self._convert_to_csv(records)
                    return {
                        "format": "csv",
                        "data": csv_data,
                        "filename": f"server_{server_id}_metrics_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}.csv"
                    }
                else:
                    # JSON æ ¼å¼
                    json_data = [record.to_dict() for record in records]
                    return {
                        "format": "json",
                        "data": json_data,
                        "filename": f"server_{server_id}_metrics_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}.json"
                    }
                    
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"åŒ¯å‡ºæ­·å²æ•¸æ“šå¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _convert_to_csv(self, records: List[SystemMetrics]) -> str:
        """è½‰æ›è¨˜éŒ„ç‚º CSV æ ¼å¼"""
        if not records:
            return ""
        
        # CSV æ¨™é¡Œ
        headers = [
            "timestamp", "cpu_usage_percent", "memory_usage_percent", 
            "disk_usage_percent", "load_average_1m", "memory_total_mb",
            "disk_total_gb", "network_bytes_sent_per_sec", "network_bytes_recv_per_sec"
        ]
        
        csv_lines = [",".join(headers)]
        
        # æ•¸æ“šè¡Œ
        for record in records:
            row = [
                record.timestamp.isoformat(),
                str(record.cpu_usage_percent or ""),
                str(record.memory_usage_percent or ""),
                str(record.disk_usage_percent or ""),
                str(record.load_average_1m or ""),
                str(record.memory_total_mb or ""),
                str(record.disk_total_gb or ""),
                str(record.network_bytes_sent_per_sec or ""),
                str(record.network_bytes_recv_per_sec or "")
            ]
            csv_lines.append(",".join(row))
        
        return "\n".join(csv_lines)


# å…¨åŸŸå¯¦ä¾‹
time_series_aggregator = TimeSeriesAggregator()
batch_aggregator = BatchDataAggregator()
historical_manager = HistoricalDataManager()


# ä¾¿åˆ©å‡½æ•¸
async def get_server_chart_data(
    server_id: int,
    metric_name: str,
    time_range: TimeRange,
    aggregation: AggregationType = AggregationType.AVERAGE
) -> ChartData:
    """å–å¾—ä¼ºæœå™¨åœ–è¡¨æ•¸æ“šçš„ä¾¿åˆ©å‡½æ•¸"""
    return await time_series_aggregator.get_metric_time_series(
        server_id, metric_name, time_range, aggregation
    )


async def get_server_dashboard_data(
    server_id: int,
    time_range: TimeRange = TimeRange.HOUR_1
) -> Dict[str, Any]:
    """å–å¾—ä¼ºæœå™¨å„€è¡¨æ¿æ•¸æ“šçš„ä¾¿åˆ©å‡½æ•¸"""
    return await time_series_aggregator.get_server_dashboard_data(server_id, time_range)


async def get_multiple_servers_dashboard_data(
    server_ids: List[int],
    time_range: TimeRange = TimeRange.HOUR_1
) -> Dict[int, Dict[str, Any]]:
    """å–å¾—å¤šå°ä¼ºæœå™¨å„€è¡¨æ¿æ•¸æ“šçš„ä¾¿åˆ©å‡½æ•¸"""
    return await batch_aggregator.generate_dashboard_data_batch(server_ids, time_range)


if __name__ == "__main__":
    # æ¸¬è©¦æ•¸æ“šèšåˆå™¨
    
    async def test_time_series_aggregation():
        """æ¸¬è©¦æ™‚åºæ•¸æ“šèšåˆ"""
        print("ğŸ“Š æ¸¬è©¦æ™‚åºæ•¸æ“šèšåˆ...")
        
        try:
            aggregator = TimeSeriesAggregator()
            
            # æ¸¬è©¦å–å¾—åœ–è¡¨æ•¸æ“š
            chart_data = await aggregator.get_metric_time_series(
                server_id=1,
                metric_name="cpu_usage_percent",
                time_range=TimeRange.HOUR_1
            )
            
            print(f"âœ… åœ–è¡¨æ•¸æ“šç”Ÿæˆå®Œæˆ:")
            print(f"  - æŒ‡æ¨™: {chart_data.metric_name}")
            print(f"  - æ™‚é–“ç¯„åœ: {chart_data.time_range.value}")
            print(f"  - æ•¸æ“šé»æ•¸é‡: {len(chart_data.time_series)}")
            print(f"  - å–®ä½: {chart_data.unit}")
            
            if chart_data.summary:
                print(f"  - ç•¶å‰å€¼: {chart_data.summary.current_value}")
                print(f"  - å¹³å‡å€¼: {chart_data.summary.average_value}")
                print(f"  - è¶¨å‹¢: {chart_data.summary.trend_direction}")
                
        except Exception as e:
            print(f"âŒ æ™‚åºæ•¸æ“šèšåˆæ¸¬è©¦å¤±æ•—: {e}")
    
    async def test_dashboard_data():
        """æ¸¬è©¦å„€è¡¨æ¿æ•¸æ“šç”Ÿæˆ"""
        print("\nğŸ›ï¸ æ¸¬è©¦å„€è¡¨æ¿æ•¸æ“šç”Ÿæˆ...")
        
        try:
            aggregator = TimeSeriesAggregator()
            
            # æ¸¬è©¦å„€è¡¨æ¿æ•¸æ“š
            dashboard_data = await aggregator.get_server_dashboard_data(
                server_id=1,
                time_range=TimeRange.HOUR_1
            )
            
            print(f"âœ… å„€è¡¨æ¿æ•¸æ“šç”Ÿæˆå®Œæˆ:")
            print(f"  - ä¼ºæœå™¨ID: {dashboard_data.get('server_id')}")
            print(f"  - æ™‚é–“ç¯„åœ: {dashboard_data.get('time_range')}")
            print(f"  - åœ–è¡¨æ•¸é‡: {len(dashboard_data.get('charts', {}))}")
            print(f"  - ç‹€æ…‹: {dashboard_data.get('status')}")
            
            current_values = dashboard_data.get('current_values', {})
            if current_values:
                print(f"  - ç•¶å‰CPU: {current_values.get('cpu_usage_percent')}%")
                print(f"  - ç•¶å‰è¨˜æ†¶é«”: {current_values.get('memory_usage_percent')}%")
                
        except Exception as e:
            print(f"âŒ å„€è¡¨æ¿æ•¸æ“šæ¸¬è©¦å¤±æ•—: {e}")
    
    async def test_batch_processing():
        """æ¸¬è©¦æ‰¹é‡è™•ç†"""
        print("\nâš¡ æ¸¬è©¦æ‰¹é‡è™•ç†...")
        
        try:
            batch_aggregator_test = BatchDataAggregator()
            
            # æ¸¬è©¦æ‰¹é‡å„€è¡¨æ¿æ•¸æ“š
            server_ids = [1, 2]  # å‡è¨­æœ‰å¤šå°ä¼ºæœå™¨
            batch_data = await batch_aggregator_test.generate_dashboard_data_batch(
                server_ids, TimeRange.HOUR_1
            )
            
            print(f"âœ… æ‰¹é‡è™•ç†å®Œæˆ:")
            print(f"  - è«‹æ±‚ä¼ºæœå™¨æ•¸: {len(server_ids)}")
            print(f"  - æˆåŠŸè™•ç†æ•¸: {len(batch_data)}")
            
            for server_id, data in batch_data.items():
                status = data.get('status', 'unknown')
                print(f"  - ä¼ºæœå™¨ {server_id}: {status}")
                
        except Exception as e:
            print(f"âŒ æ‰¹é‡è™•ç†æ¸¬è©¦å¤±æ•—: {e}")
    
    async def test_complete():
        """å®Œæ•´æ¸¬è©¦"""
        print("=" * 50)
        print("ğŸ§ª CWatcher æ™‚åºæ•¸æ“šèšåˆç³»çµ±æ¸¬è©¦")
        print("=" * 50)
        
        await test_time_series_aggregation()
        await test_dashboard_data()
        await test_batch_processing()
        
        print("\nâœ… æ™‚åºæ•¸æ“šèšåˆç³»çµ±æ¸¬è©¦å®Œæˆ")
    
    # åŸ·è¡Œæ¸¬è©¦
    import asyncio
    asyncio.run(test_complete())