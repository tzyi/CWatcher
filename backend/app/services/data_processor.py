"""
CWatcher æ•¸æ“šè™•ç†èˆ‡å­˜å„²æœå‹™

å°ˆé–€è² è²¬ç›£æ§æ•¸æ“šçš„æ¨™æº–åŒ–ã€æ‰¹é‡å­˜å„²ã€èšåˆçµ±è¨ˆå’Œæ•¸æ“šç®¡ç†
æ”¯æ´é«˜æ•ˆçš„æ™‚åºæ•¸æ“šè™•ç†å’Œåœ–è¡¨æ•¸æ“šç”Ÿæˆ
"""

import asyncio
import logging
import time
import json
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from sqlalchemy.orm import Session
from sqlalchemy import text, func, desc, asc
from contextlib import asynccontextmanager

from app.core.deps import get_db
from app.models.system_metrics import SystemMetrics
from app.models.server import Server
from app.services.monitoring_collector import MonitoringData, MetricType, AlertLevel
from app.core.config import settings

# è¨­å®šæ—¥èªŒ
logger = logging.getLogger(__name__)


class DataValidationError(Exception):
    """æ•¸æ“šé©—è­‰éŒ¯èª¤"""
    pass


class StorageError(Exception):
    """å­˜å„²éŒ¯èª¤"""
    pass


@dataclass
class ProcessingStats:
    """æ•¸æ“šè™•ç†çµ±è¨ˆ"""
    total_records: int = 0
    valid_records: int = 0
    invalid_records: int = 0
    duplicate_records: int = 0
    processing_time: float = 0.0
    storage_time: float = 0.0
    errors: List[str] = field(default_factory=list)


@dataclass
class StandardizedMetrics:
    """æ¨™æº–åŒ–ç›£æ§æ•¸æ“š"""
    server_id: int
    timestamp: datetime
    
    # CPU æŒ‡æ¨™ (æ¨™æº–åŒ–ç‚ºç™¾åˆ†æ¯”å’Œæ•¸å€¼)
    cpu_usage_percent: Optional[float] = None
    cpu_user_percent: Optional[float] = None
    cpu_system_percent: Optional[float] = None
    cpu_idle_percent: Optional[float] = None
    cpu_iowait_percent: Optional[float] = None
    cpu_count: Optional[int] = None
    cpu_frequency_mhz: Optional[float] = None
    load_average_1m: Optional[float] = None
    load_average_5m: Optional[float] = None
    load_average_15m: Optional[float] = None
    
    # è¨˜æ†¶é«”æŒ‡æ¨™ (æ¨™æº–åŒ–ç‚º MB)
    memory_total_mb: Optional[int] = None
    memory_used_mb: Optional[int] = None
    memory_available_mb: Optional[int] = None
    memory_free_mb: Optional[int] = None
    memory_cached_mb: Optional[int] = None
    memory_buffers_mb: Optional[int] = None
    memory_usage_percent: Optional[float] = None
    
    # Swap æŒ‡æ¨™ (æ¨™æº–åŒ–ç‚º MB)
    swap_total_mb: Optional[int] = None
    swap_used_mb: Optional[int] = None
    swap_free_mb: Optional[int] = None
    swap_usage_percent: Optional[float] = None
    
    # ç£ç¢ŸæŒ‡æ¨™ (æ¨™æº–åŒ–ç‚º GB)
    disk_total_gb: Optional[float] = None
    disk_used_gb: Optional[float] = None
    disk_free_gb: Optional[float] = None
    disk_usage_percent: Optional[float] = None
    
    # ç£ç¢Ÿ I/O æŒ‡æ¨™ (bytes/s)
    disk_read_bytes_per_sec: Optional[int] = None
    disk_write_bytes_per_sec: Optional[int] = None
    disk_read_iops: Optional[int] = None
    disk_write_iops: Optional[int] = None
    
    # ç¶²è·¯æŒ‡æ¨™ (bytes/s)
    network_interface: Optional[str] = None
    network_bytes_sent_per_sec: Optional[int] = None
    network_bytes_recv_per_sec: Optional[int] = None
    network_packets_sent_per_sec: Optional[int] = None
    network_packets_recv_per_sec: Optional[int] = None
    network_errors_in: Optional[int] = None
    network_errors_out: Optional[int] = None
    
    # ç³»çµ±æŒ‡æ¨™
    uptime_seconds: Optional[int] = None
    processes_total: Optional[int] = None
    processes_running: Optional[int] = None
    processes_sleeping: Optional[int] = None
    processes_zombie: Optional[int] = None
    
    # ç‹€æ…‹æŒ‡æ¨™
    collection_duration_ms: Optional[int] = None
    collection_success: bool = True
    error_message: Optional[str] = None
    
    def to_system_metrics(self) -> SystemMetrics:
        """è½‰æ›ç‚º SystemMetrics æ¨¡å‹"""
        return SystemMetrics(
            server_id=self.server_id,
            timestamp=self.timestamp,
            
            # CPU æŒ‡æ¨™
            cpu_usage_percent=self.cpu_usage_percent,
            cpu_user_percent=self.cpu_user_percent,
            cpu_system_percent=self.cpu_system_percent,
            cpu_idle_percent=self.cpu_idle_percent,
            cpu_iowait_percent=self.cpu_iowait_percent,
            cpu_count=self.cpu_count,
            cpu_frequency_mhz=self.cpu_frequency_mhz,
            load_average_1m=self.load_average_1m,
            load_average_5m=self.load_average_5m,
            load_average_15m=self.load_average_15m,
            
            # è¨˜æ†¶é«”æŒ‡æ¨™
            memory_total_mb=self.memory_total_mb,
            memory_used_mb=self.memory_used_mb,
            memory_available_mb=self.memory_available_mb,
            memory_free_mb=self.memory_free_mb,
            memory_cached_mb=self.memory_cached_mb,
            memory_buffers_mb=self.memory_buffers_mb,
            memory_usage_percent=self.memory_usage_percent,
            
            # Swap æŒ‡æ¨™
            swap_total_mb=self.swap_total_mb,
            swap_used_mb=self.swap_used_mb,
            swap_free_mb=self.swap_free_mb,
            swap_usage_percent=self.swap_usage_percent,
            
            # ç£ç¢ŸæŒ‡æ¨™
            disk_total_gb=self.disk_total_gb,
            disk_used_gb=self.disk_used_gb,
            disk_free_gb=self.disk_free_gb,
            disk_usage_percent=self.disk_usage_percent,
            
            # ç£ç¢Ÿ I/O æŒ‡æ¨™
            disk_read_bytes_per_sec=self.disk_read_bytes_per_sec,
            disk_write_bytes_per_sec=self.disk_write_bytes_per_sec,
            disk_read_iops=self.disk_read_iops,
            disk_write_iops=self.disk_write_iops,
            
            # ç¶²è·¯æŒ‡æ¨™
            network_interface=self.network_interface,
            network_bytes_sent_per_sec=self.network_bytes_sent_per_sec,
            network_bytes_recv_per_sec=self.network_bytes_recv_per_sec,
            network_packets_sent_per_sec=self.network_packets_sent_per_sec,
            network_packets_recv_per_sec=self.network_packets_recv_per_sec,
            network_errors_in=self.network_errors_in,
            network_errors_out=self.network_errors_out,
            
            # ç³»çµ±æŒ‡æ¨™
            uptime_seconds=self.uptime_seconds,
            processes_total=self.processes_total,
            processes_running=self.processes_running,
            processes_sleeping=self.processes_sleeping,
            processes_zombie=self.processes_zombie,
            
            # ç‹€æ…‹æŒ‡æ¨™
            collection_duration_ms=self.collection_duration_ms,
            collection_success=self.collection_success,
            error_message=self.error_message
        )


class DataStandardizer:
    """æ•¸æ“šæ¨™æº–åŒ–è™•ç†å™¨"""
    
    @staticmethod
    def standardize_monitoring_data(
        server_id: int,
        monitoring_data: Dict[MetricType, MonitoringData]
    ) -> StandardizedMetrics:
        """
        æ¨™æº–åŒ–ç›£æ§æ•¸æ“š
        
        å°‡æ”¶é›†å™¨çš„åŸå§‹æ•¸æ“šè½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
        è™•ç†å–®ä½è½‰æ›ã€æ•¸æ“šé©—è­‰å’Œæ ¼å¼çµ±ä¸€
        """
        try:
            # å»ºç«‹æ¨™æº–åŒ–æ•¸æ“šå°è±¡
            standardized = StandardizedMetrics(
                server_id=server_id,
                timestamp=datetime.now()
            )
            
            # è™•ç† CPU æ•¸æ“š
            if MetricType.CPU in monitoring_data:
                cpu_data = monitoring_data[MetricType.CPU]
                if cpu_data.data and cpu_data.data.get("collection_status") == "success":
                    # æå–ä¸¦é©—è­‰ CPU æ•¸æ“š
                    standardized.cpu_usage_percent = DataStandardizer._validate_percentage(
                        cpu_data.data.get("usage_percent")
                    )
                    standardized.cpu_count = DataStandardizer._validate_positive_int(
                        cpu_data.data.get("core_count")
                    )
                    standardized.cpu_frequency_mhz = DataStandardizer._validate_positive_float(
                        cpu_data.data.get("frequency_mhz")
                    )
                    
                    # è² è¼‰å¹³å‡å€¼
                    load_avg = cpu_data.data.get("load_average", {})
                    standardized.load_average_1m = DataStandardizer._validate_positive_float(
                        load_avg.get("1min")
                    )
                    standardized.load_average_5m = DataStandardizer._validate_positive_float(
                        load_avg.get("5min")
                    )
                    standardized.load_average_15m = DataStandardizer._validate_positive_float(
                        load_avg.get("15min")
                    )
                    
                    # æ”¶é›†æ™‚é–“
                    standardized.collection_duration_ms = int(cpu_data.collection_time * 1000)
            
            # è™•ç†è¨˜æ†¶é«”æ•¸æ“š
            if MetricType.MEMORY in monitoring_data:
                memory_data = monitoring_data[MetricType.MEMORY]
                if memory_data.data and memory_data.data.get("collection_status") == "success":
                    # è½‰æ› bytes åˆ° MB
                    standardized.memory_total_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("total_bytes")
                    )
                    standardized.memory_used_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("used_bytes")
                    )
                    standardized.memory_available_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("available_bytes")
                    )
                    standardized.memory_free_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("free_bytes")
                    )
                    standardized.memory_cached_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("cached_bytes")
                    )
                    standardized.memory_buffers_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("buffers_bytes")
                    )
                    standardized.memory_usage_percent = DataStandardizer._validate_percentage(
                        memory_data.data.get("usage_percent")
                    )
                    
                    # Swap æ•¸æ“š
                    standardized.swap_total_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("swap_total_bytes")
                    )
                    standardized.swap_used_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("swap_used_bytes")
                    )
                    standardized.swap_free_mb = DataStandardizer._bytes_to_mb(
                        memory_data.data.get("swap_free_bytes")
                    )
                    standardized.swap_usage_percent = DataStandardizer._validate_percentage(
                        memory_data.data.get("swap_usage_percent")
                    )
            
            # è™•ç†ç£ç¢Ÿæ•¸æ“š
            if MetricType.DISK in monitoring_data:
                disk_data = monitoring_data[MetricType.DISK]
                if disk_data.data and disk_data.data.get("collection_status") == "success":
                    # è½‰æ› bytes åˆ° GB
                    standardized.disk_total_gb = DataStandardizer._bytes_to_gb(
                        disk_data.data.get("total_space_bytes")
                    )
                    standardized.disk_used_gb = DataStandardizer._bytes_to_gb(
                        disk_data.data.get("used_space_bytes")
                    )
                    standardized.disk_free_gb = DataStandardizer._bytes_to_gb(
                        disk_data.data.get("free_space_bytes")
                    )
                    standardized.disk_usage_percent = DataStandardizer._validate_percentage(
                        disk_data.data.get("overall_usage_percent")
                    )
                    
                    # I/O çµ±è¨ˆ (å–ä¸»è¦è¨­å‚™çš„å¹³å‡å€¼)
                    io_stats = disk_data.data.get("io_stats", {})
                    if io_stats:
                        total_read_kbps = sum(stats.get("read_kb_per_sec", 0) for stats in io_stats.values())
                        total_write_kbps = sum(stats.get("write_kb_per_sec", 0) for stats in io_stats.values())
                        
                        standardized.disk_read_bytes_per_sec = int(total_read_kbps * 1024) if total_read_kbps > 0 else None
                        standardized.disk_write_bytes_per_sec = int(total_write_kbps * 1024) if total_write_kbps > 0 else None
                        
                        # IOPS çµ±è¨ˆ
                        total_read_iops = sum(stats.get("reads_per_sec", 0) for stats in io_stats.values())
                        total_write_iops = sum(stats.get("writes_per_sec", 0) for stats in io_stats.values())
                        
                        standardized.disk_read_iops = int(total_read_iops) if total_read_iops > 0 else None
                        standardized.disk_write_iops = int(total_write_iops) if total_write_iops > 0 else None
            
            # è™•ç†ç¶²è·¯æ•¸æ“š
            if MetricType.NETWORK in monitoring_data:
                network_data = monitoring_data[MetricType.NETWORK]
                if network_data.data and network_data.data.get("collection_status") == "success":
                    # å–ä¸»è¦ç¶²è·¯ä»‹é¢çš„æ•¸æ“š
                    interfaces = network_data.data.get("interfaces", {})
                    
                    # æ‰¾åˆ°æµé‡æœ€å¤§çš„ä»‹é¢ (æ’é™¤ lo)
                    main_interface = None
                    max_traffic = 0
                    
                    for iface, stats in interfaces.items():
                        if iface == "lo":
                            continue
                        
                        traffic = stats.get("rx_bytes", 0) + stats.get("tx_bytes", 0)
                        if traffic > max_traffic:
                            max_traffic = traffic
                            main_interface = iface
                    
                    if main_interface and main_interface in interfaces:
                        iface_stats = interfaces[main_interface]
                        
                        standardized.network_interface = main_interface
                        standardized.network_bytes_sent_per_sec = DataStandardizer._validate_positive_int(
                            iface_stats.get("tx_speed_bps")
                        )
                        standardized.network_bytes_recv_per_sec = DataStandardizer._validate_positive_int(
                            iface_stats.get("rx_speed_bps")
                        )
                        standardized.network_errors_in = DataStandardizer._validate_positive_int(
                            iface_stats.get("rx_errors")
                        )
                        standardized.network_errors_out = DataStandardizer._validate_positive_int(
                            iface_stats.get("tx_errors")
                        )
            
            # è¨­å®šæ”¶é›†æˆåŠŸç‹€æ…‹
            standardized.collection_success = all(
                data.alert_level != AlertLevel.UNKNOWN 
                for data in monitoring_data.values()
            )
            
            # æ”¶é›†éŒ¯èª¤è¨Šæ¯
            error_messages = [
                data.alert_message for data in monitoring_data.values() 
                if data.alert_message and data.alert_level == AlertLevel.UNKNOWN
            ]
            if error_messages:
                standardized.error_message = "; ".join(error_messages)
            
            return standardized
            
        except Exception as e:
            logger.error(f"æ¨™æº–åŒ–ç›£æ§æ•¸æ“šå¤±æ•—: {e}")
            # å›å‚³åŸºæœ¬çš„éŒ¯èª¤è¨˜éŒ„
            return StandardizedMetrics(
                server_id=server_id,
                timestamp=datetime.now(),
                collection_success=False,
                error_message=f"æ•¸æ“šæ¨™æº–åŒ–å¤±æ•—: {str(e)}"
            )
    
    @staticmethod
    def _validate_percentage(value: Any) -> Optional[float]:
        """é©—è­‰ç™¾åˆ†æ¯”æ•¸å€¼ (0-100)"""
        if value is None:
            return None
        try:
            val = float(value)
            return max(0.0, min(100.0, val))
        except (ValueError, TypeError):
            return None
    
    @staticmethod
    def _validate_positive_int(value: Any) -> Optional[int]:
        """é©—è­‰æ­£æ•´æ•¸"""
        if value is None:
            return None
        try:
            val = int(float(value))
            return max(0, val)
        except (ValueError, TypeError):
            return None
    
    @staticmethod
    def _validate_positive_float(value: Any) -> Optional[float]:
        """é©—è­‰æ­£æµ®é»æ•¸"""
        if value is None:
            return None
        try:
            val = float(value)
            return max(0.0, val)
        except (ValueError, TypeError):
            return None
    
    @staticmethod
    def _bytes_to_mb(value: Any) -> Optional[int]:
        """è½‰æ› bytes åˆ° MB"""
        if value is None:
            return None
        try:
            val = int(float(value))
            return max(0, val // (1024 * 1024))
        except (ValueError, TypeError):
            return None
    
    @staticmethod
    def _bytes_to_gb(value: Any) -> Optional[float]:
        """è½‰æ› bytes åˆ° GB"""
        if value is None:
            return None
        try:
            val = float(value)
            return max(0.0, round(val / (1024 * 1024 * 1024), 2))
        except (ValueError, TypeError):
            return None


class BatchStorageManager:
    """æ‰¹é‡å­˜å„²ç®¡ç†å™¨"""
    
    def __init__(self, batch_size: int = 100, flush_interval: int = 30):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self._batch_buffer: List[StandardizedMetrics] = []
        self._last_flush_time = time.time()
        self._flush_lock = asyncio.Lock()
    
    async def add_metrics(self, metrics: Union[StandardizedMetrics, List[StandardizedMetrics]]) -> ProcessingStats:
        """
        æ·»åŠ æŒ‡æ¨™åˆ°æ‰¹é‡ç·©è¡å€
        é”åˆ°æ‰¹é‡å¤§å°æˆ–æ™‚é–“é–“éš”æ™‚è‡ªå‹•åˆ·æ–°
        """
        stats = ProcessingStats()
        
        try:
            # æ¨™æº–åŒ–è¼¸å…¥
            if isinstance(metrics, StandardizedMetrics):
                metrics = [metrics]
            
            async with self._flush_lock:
                # æ·»åŠ åˆ°ç·©è¡å€
                self._batch_buffer.extend(metrics)
                stats.total_records = len(metrics)
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
                should_flush = (
                    len(self._batch_buffer) >= self.batch_size or
                    time.time() - self._last_flush_time >= self.flush_interval
                )
                
                if should_flush:
                    flush_stats = await self._flush_batch()
                    stats.valid_records = flush_stats.valid_records
                    stats.invalid_records = flush_stats.invalid_records
                    stats.duplicate_records = flush_stats.duplicate_records
                    stats.storage_time = flush_stats.storage_time
                    stats.errors.extend(flush_stats.errors)
                else:
                    stats.valid_records = len(metrics)
            
            return stats
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ·»åŠ æŒ‡æ¨™å¤±æ•—: {e}")
            stats.errors.append(str(e))
            return stats
    
    async def flush(self) -> ProcessingStats:
        """å¼·åˆ¶åˆ·æ–°ç·©è¡å€"""
        async with self._flush_lock:
            return await self._flush_batch()
    
    async def _flush_batch(self) -> ProcessingStats:
        """å…§éƒ¨æ‰¹é‡åˆ·æ–°å¯¦ç¾"""
        stats = ProcessingStats()
        
        if not self._batch_buffer:
            return stats
        
        start_time = time.time()
        
        try:
            # é©—è­‰æ•¸æ“š
            valid_metrics = []
            for metric in self._batch_buffer:
                try:
                    self._validate_metrics(metric)
                    valid_metrics.append(metric)
                    stats.valid_records += 1
                except DataValidationError as e:
                    stats.invalid_records += 1
                    stats.errors.append(f"æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
                    logger.warning(f"æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
            
            # æ‰¹é‡å­˜å„²
            if valid_metrics:
                stored_count = await self._batch_insert_metrics(valid_metrics)
                stats.duplicate_records = len(valid_metrics) - stored_count
            
            # æ¸…ç©ºç·©è¡å€
            stats.total_records = len(self._batch_buffer)
            self._batch_buffer.clear()
            self._last_flush_time = time.time()
            
            stats.storage_time = time.time() - start_time
            
            logger.info(f"æ‰¹é‡å­˜å„²å®Œæˆ: {stats.valid_records} æˆåŠŸ, {stats.invalid_records} å¤±æ•—, "
                       f"{stats.duplicate_records} é‡è¤‡, è€—æ™‚ {stats.storage_time:.2f}s")
            
            return stats
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ·æ–°å¤±æ•—: {e}")
            stats.errors.append(str(e))
            return stats
    
    def _validate_metrics(self, metrics: StandardizedMetrics):
        """é©—è­‰æ•¸æ“šå®Œæ•´æ€§"""
        if not metrics.server_id:
            raise DataValidationError("ç¼ºå°‘ server_id")
        
        if not metrics.timestamp:
            raise DataValidationError("ç¼ºå°‘ timestamp")
        
        # æª¢æŸ¥æ™‚é–“æˆ³æ˜¯å¦åˆç† (ä¸èƒ½æ˜¯æœªä¾†æ™‚é–“ï¼Œä¸èƒ½è¶…é24å°æ™‚å‰)
        now = datetime.now()
        if metrics.timestamp > now:
            raise DataValidationError(f"æ™‚é–“æˆ³ä¸èƒ½æ˜¯æœªä¾†æ™‚é–“: {metrics.timestamp}")
        
        if metrics.timestamp < now - timedelta(hours=24):
            raise DataValidationError(f"æ™‚é–“æˆ³éæ–¼ä¹…é : {metrics.timestamp}")
        
        # é©—è­‰ç™¾åˆ†æ¯”æ•¸å€¼ç¯„åœ
        percentage_fields = [
            'cpu_usage_percent', 'cpu_user_percent', 'cpu_system_percent',
            'cpu_idle_percent', 'cpu_iowait_percent', 'memory_usage_percent',
            'swap_usage_percent', 'disk_usage_percent'
        ]
        
        for field in percentage_fields:
            value = getattr(metrics, field)
            if value is not None and (value < 0 or value > 100):
                raise DataValidationError(f"{field} æ•¸å€¼ç¯„åœéŒ¯èª¤: {value}")
    
    async def _batch_insert_metrics(self, metrics: List[StandardizedMetrics]) -> int:
        """æ‰¹é‡æ’å…¥æŒ‡æ¨™æ•¸æ“š"""
        if not metrics:
            return 0
        
        try:
            # ä½¿ç”¨æ•¸æ“šåº«æœƒè©±
            async with self._get_db_session() as db:
                # è½‰æ›ç‚º SystemMetrics æ¨¡å‹
                db_metrics = [metric.to_system_metrics() for metric in metrics]
                
                # æ‰¹é‡æ’å…¥
                db.add_all(db_metrics)
                await db.commit()
                
                return len(db_metrics)
                
        except Exception as e:
            logger.error(f"æ‰¹é‡æ’å…¥å¤±æ•—: {e}")
            raise StorageError(f"æ•¸æ“šå­˜å„²å¤±æ•—: {e}")
    
    @asynccontextmanager
    async def _get_db_session(self):
        """å–å¾—æ•¸æ“šåº«æœƒè©±"""
        db = next(get_db())
        try:
            yield db
        finally:
            db.close()


class DataProcessor:
    """æ•¸æ“šè™•ç†ä¸»æœå‹™"""
    
    def __init__(self):
        self.standardizer = DataStandardizer()
        self.storage_manager = BatchStorageManager(
            batch_size=settings.BATCH_SIZE if hasattr(settings, 'BATCH_SIZE') else 100,
            flush_interval=settings.FLUSH_INTERVAL if hasattr(settings, 'FLUSH_INTERVAL') else 30
        )
        self._processing_stats = ProcessingStats()
    
    async def process_monitoring_data(
        self, 
        server_id: int, 
        monitoring_data: Dict[MetricType, MonitoringData]
    ) -> ProcessingStats:
        """
        è™•ç†ç›£æ§æ•¸æ“šçš„ä¸»è¦å…¥å£
        
        1. æ¨™æº–åŒ–æ•¸æ“šæ ¼å¼
        2. é©—è­‰æ•¸æ“šå®Œæ•´æ€§
        3. æ‰¹é‡å­˜å„²åˆ°æ•¸æ“šåº«
        """
        start_time = time.time()
        
        try:
            # æ¨™æº–åŒ–æ•¸æ“š
            standardized_metrics = self.standardizer.standardize_monitoring_data(
                server_id, monitoring_data
            )
            
            # æ‰¹é‡å­˜å„²
            storage_stats = await self.storage_manager.add_metrics(standardized_metrics)
            
            # æ›´æ–°çµ±è¨ˆ
            storage_stats.processing_time = time.time() - start_time
            
            return storage_stats
            
        except Exception as e:
            logger.error(f"è™•ç†ç›£æ§æ•¸æ“šå¤±æ•—: {e}")
            error_stats = ProcessingStats()
            error_stats.total_records = 1
            error_stats.invalid_records = 1
            error_stats.processing_time = time.time() - start_time
            error_stats.errors.append(str(e))
            return error_stats
    
    async def batch_process_monitoring_data(
        self, 
        server_data_list: List[Tuple[int, Dict[MetricType, MonitoringData]]]
    ) -> ProcessingStats:
        """æ‰¹é‡è™•ç†å¤šå°ä¼ºæœå™¨çš„ç›£æ§æ•¸æ“š"""
        start_time = time.time()
        combined_stats = ProcessingStats()
        
        try:
            # ä¸¦è¡Œè™•ç†æ‰€æœ‰ä¼ºæœå™¨æ•¸æ“š
            tasks = [
                self.process_monitoring_data(server_id, monitoring_data)
                for server_id, monitoring_data in server_data_list
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # åˆä½µçµ±è¨ˆçµæœ
            for result in results:
                if isinstance(result, ProcessingStats):
                    combined_stats.total_records += result.total_records
                    combined_stats.valid_records += result.valid_records
                    combined_stats.invalid_records += result.invalid_records
                    combined_stats.duplicate_records += result.duplicate_records
                    combined_stats.storage_time += result.storage_time
                    combined_stats.errors.extend(result.errors)
                elif isinstance(result, Exception):
                    combined_stats.invalid_records += 1
                    combined_stats.errors.append(str(result))
            
            combined_stats.processing_time = time.time() - start_time
            
            logger.info(f"æ‰¹é‡è™•ç†å®Œæˆ: {len(server_data_list)} å°ä¼ºæœå™¨, "
                       f"{combined_stats.valid_records} æˆåŠŸ, {combined_stats.invalid_records} å¤±æ•—")
            
            return combined_stats
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è™•ç†ç›£æ§æ•¸æ“šå¤±æ•—: {e}")
            combined_stats.invalid_records = len(server_data_list)
            combined_stats.processing_time = time.time() - start_time
            combined_stats.errors.append(str(e))
            return combined_stats
    
    async def flush_all_data(self) -> ProcessingStats:
        """å¼·åˆ¶åˆ·æ–°æ‰€æœ‰ç·©è¡æ•¸æ“š"""
        return await self.storage_manager.flush()
    
    def get_processing_stats(self) -> ProcessingStats:
        """å–å¾—è™•ç†çµ±è¨ˆ"""
        return self._processing_stats


# å…¨åŸŸæ•¸æ“šè™•ç†å™¨å¯¦ä¾‹
data_processor = DataProcessor()


# ä¾¿åˆ©å‡½æ•¸
async def process_server_monitoring_data(
    server_id: int, 
    monitoring_data: Dict[MetricType, MonitoringData]
) -> ProcessingStats:
    """è™•ç†å–®å°ä¼ºæœå™¨ç›£æ§æ•¸æ“šçš„ä¾¿åˆ©å‡½æ•¸"""
    return await data_processor.process_monitoring_data(server_id, monitoring_data)


async def batch_process_servers_monitoring_data(
    server_data_list: List[Tuple[int, Dict[MetricType, MonitoringData]]]
) -> ProcessingStats:
    """æ‰¹é‡è™•ç†å¤šå°ä¼ºæœå™¨ç›£æ§æ•¸æ“šçš„ä¾¿åˆ©å‡½æ•¸"""
    return await data_processor.batch_process_monitoring_data(server_data_list)


async def flush_monitoring_data() -> ProcessingStats:
    """åˆ·æ–°ç›£æ§æ•¸æ“šç·©è¡å€çš„ä¾¿åˆ©å‡½æ•¸"""
    return await data_processor.flush_all_data()


if __name__ == "__main__":
    # æ¸¬è©¦æ•¸æ“šè™•ç†å™¨
    
    async def test_data_standardization():
        """æ¸¬è©¦æ•¸æ“šæ¨™æº–åŒ–"""
        print("ğŸ”§ æ¸¬è©¦æ•¸æ“šæ¨™æº–åŒ–...")
        
        # æ¨¡æ“¬ç›£æ§æ•¸æ“š
        from app.services.monitoring_collector import MonitoringData, MetricType, AlertLevel
        
        mock_cpu_data = MonitoringData(
            metric_type=MetricType.CPU,
            server_id=1,
            data={
                "collection_status": "success",
                "usage_percent": 45.2,
                "core_count": 4,
                "frequency_mhz": 2400.0,
                "load_average": {"1min": 1.5, "5min": 1.2, "15min": 0.8}
            },
            alert_level=AlertLevel.OK,
            collection_time=0.5
        )
        
        mock_memory_data = MonitoringData(
            metric_type=MetricType.MEMORY,
            server_id=1,
            data={
                "collection_status": "success",
                "total_bytes": 8589934592,  # 8GB
                "used_bytes": 5497558138,   # ~5.1GB
                "available_bytes": 3092376454,  # ~2.9GB
                "usage_percent": 64.0,
                "swap_total_bytes": 2147483648,  # 2GB
                "swap_used_bytes": 0,
                "swap_usage_percent": 0.0
            },
            alert_level=AlertLevel.OK,
            collection_time=0.3
        )
        
        monitoring_data = {
            MetricType.CPU: mock_cpu_data,
            MetricType.MEMORY: mock_memory_data
        }
        
        # æ¸¬è©¦æ¨™æº–åŒ–
        standardized = DataStandardizer.standardize_monitoring_data(1, monitoring_data)
        
        print(f"âœ… æ¨™æº–åŒ–å®Œæˆ:")
        print(f"  - CPUä½¿ç”¨ç‡: {standardized.cpu_usage_percent}%")
        print(f"  - CPUæ ¸å¿ƒæ•¸: {standardized.cpu_count}")
        print(f"  - è¨˜æ†¶é«”ç¸½é‡: {standardized.memory_total_mb}MB")
        print(f"  - è¨˜æ†¶é«”ä½¿ç”¨ç‡: {standardized.memory_usage_percent}%")
        print(f"  - æ”¶é›†ç‹€æ…‹: {standardized.collection_success}")
    
    async def test_batch_storage():
        """æ¸¬è©¦æ‰¹é‡å­˜å„²"""
        print("\nğŸ’¾ æ¸¬è©¦æ‰¹é‡å­˜å„²...")
        
        try:
            # å»ºç«‹æ¸¬è©¦æ•¸æ“š
            test_metrics = []
            for i in range(5):
                metrics = StandardizedMetrics(
                    server_id=1,
                    timestamp=datetime.now() - timedelta(seconds=i*30),
                    cpu_usage_percent=40.0 + i * 5,
                    memory_usage_percent=60.0 + i * 2,
                    collection_success=True
                )
                test_metrics.append(metrics)
            
            # æ¸¬è©¦æ‰¹é‡å­˜å„²
            storage_manager = BatchStorageManager(batch_size=3)
            stats = await storage_manager.add_metrics(test_metrics)
            
            print(f"âœ… æ‰¹é‡å­˜å„²å®Œæˆ:")
            print(f"  - ç¸½è¨˜éŒ„æ•¸: {stats.total_records}")
            print(f"  - æœ‰æ•ˆè¨˜éŒ„: {stats.valid_records}")
            print(f"  - ç„¡æ•ˆè¨˜éŒ„: {stats.invalid_records}")
            print(f"  - å­˜å„²æ™‚é–“: {stats.storage_time:.3f}s")
            
            if stats.errors:
                print(f"  - éŒ¯èª¤: {stats.errors}")
                
        except ImportError:
            print("âš ï¸ ç„¡æ³•é€£æ¥æ•¸æ“šåº«ï¼Œè·³éå­˜å„²æ¸¬è©¦")
        except Exception as e:
            print(f"âŒ æ‰¹é‡å­˜å„²æ¸¬è©¦å¤±æ•—: {e}")
    
    async def test_data_processor():
        """æ¸¬è©¦å®Œæ•´æ•¸æ“šè™•ç†æµç¨‹"""
        print("\nğŸš€ æ¸¬è©¦å®Œæ•´æ•¸æ“šè™•ç†æµç¨‹...")
        
        # æ¨¡æ“¬ç›£æ§æ•¸æ“š
        from app.services.monitoring_collector import MonitoringData, MetricType, AlertLevel
        
        mock_monitoring_data = {
            MetricType.CPU: MonitoringData(
                metric_type=MetricType.CPU,
                server_id=1,
                data={
                    "collection_status": "success",
                    "usage_percent": 55.0,
                    "core_count": 8,
                    "frequency_mhz": 3200.0
                },
                alert_level=AlertLevel.OK
            ),
            MetricType.MEMORY: MonitoringData(
                metric_type=MetricType.MEMORY,
                server_id=1,
                data={
                    "collection_status": "success",
                    "total_bytes": 17179869184,  # 16GB
                    "used_bytes": 12884901888,   # 12GB
                    "usage_percent": 75.0
                },
                alert_level=AlertLevel.WARNING
            )
        }
        
        # æ¸¬è©¦è™•ç†
        processor = DataProcessor()
        stats = await processor.process_monitoring_data(1, mock_monitoring_data)
        
        print(f"âœ… æ•¸æ“šè™•ç†å®Œæˆ:")
        print(f"  - è™•ç†æ™‚é–“: {stats.processing_time:.3f}s")
        print(f"  - ç¸½è¨˜éŒ„æ•¸: {stats.total_records}")
        print(f"  - æœ‰æ•ˆè¨˜éŒ„: {stats.valid_records}")
        
        if stats.errors:
            print(f"  - éŒ¯èª¤: {stats.errors}")
    
    async def test_complete():
        """å®Œæ•´æ¸¬è©¦"""
        print("=" * 50)
        print("ğŸ§ª CWatcher æ•¸æ“šè™•ç†èˆ‡å­˜å„²æœå‹™æ¸¬è©¦")
        print("=" * 50)
        
        await test_data_standardization()
        await test_batch_storage()
        await test_data_processor()
        
        print("\nâœ… æ•¸æ“šè™•ç†èˆ‡å­˜å„²æœå‹™æ¸¬è©¦å®Œæˆ")
    
    # åŸ·è¡Œæ¸¬è©¦
    import asyncio
    asyncio.run(test_complete())