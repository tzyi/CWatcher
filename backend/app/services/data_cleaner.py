"""
CWatcher æ•¸æ“šæ¸…ç†èˆ‡æ­¸æª”æœå‹™

å°ˆé–€è² è²¬èˆŠæ•¸æ“šæ¸…ç†ã€æ•¸æ“šæ­¸æª”å’Œå„²å­˜ç©ºé–“ç›£æ§
æ”¯æ´è‡ªå‹•åŒ–æ¸…ç†ç­–ç•¥å’Œæ•¸æ“šç”Ÿå‘½é€±æœŸç®¡ç†
"""

import asyncio
import logging
import os
import shutil
import time
import json
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from sqlalchemy.orm import Session
from sqlalchemy import text, func, desc, and_, or_, delete
from contextlib import asynccontextmanager

from core.deps import get_db
from db.base import get_sync_db
from models.system_metrics import SystemMetrics
from models.server import Server
from core.config import settings

# è¨­å®šæ—¥èªŒ
logger = logging.getLogger(__name__)


class CleanupLevel(Enum):
    """æ¸…ç†ç­‰ç´š"""
    BASIC = "basic"       # åŸºæœ¬æ¸…ç†ï¼ˆ30å¤©ä»¥ä¸Šï¼‰
    AGGRESSIVE = "aggressive"  # ç©æ¥µæ¸…ç†ï¼ˆ7å¤©ä»¥ä¸Šï¼‰
    EMERGENCY = "emergency"    # ç·Šæ€¥æ¸…ç†ï¼ˆ1å¤©ä»¥ä¸Šï¼‰


class DataType(Enum):
    """æ•¸æ“šé¡å‹"""
    METRICS = "metrics"   # ç›£æ§æ•¸æ“š
    LOGS = "logs"        # æ—¥èªŒæ•¸æ“š
    ARCHIVES = "archives" # æ­¸æª”æ•¸æ“š


@dataclass
class CleanupStats:
    """æ¸…ç†çµ±è¨ˆ"""
    cleaned_records: int = 0
    cleaned_size_bytes: int = 0
    archived_records: int = 0
    archived_size_bytes: int = 0
    cleanup_time: float = 0.0
    errors: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "cleaned_records": self.cleaned_records,
            "cleaned_size_mb": round(self.cleaned_size_bytes / (1024 * 1024), 2),
            "archived_records": self.archived_records,
            "archived_size_mb": round(self.archived_size_bytes / (1024 * 1024), 2),
            "cleanup_time": self.cleanup_time,
            "errors": self.errors
        }


@dataclass
class StorageInfo:
    """å„²å­˜ç©ºé–“è³‡è¨Š"""
    total_space_bytes: int
    used_space_bytes: int
    free_space_bytes: int
    usage_percentage: float
    database_size_bytes: int = 0
    archive_size_bytes: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "total_space_gb": round(self.total_space_bytes / (1024**3), 2),
            "used_space_gb": round(self.used_space_bytes / (1024**3), 2),
            "free_space_gb": round(self.free_space_bytes / (1024**3), 2),
            "usage_percentage": self.usage_percentage,
            "database_size_mb": round(self.database_size_bytes / (1024**2), 2),
            "archive_size_mb": round(self.archive_size_bytes / (1024**2), 2)
        }


@dataclass
class CleanupPolicy:
    """æ¸…ç†ç­–ç•¥"""
    name: str
    retention_days: int
    enabled: bool = True
    archive_before_delete: bool = True
    batch_size: int = 1000
    
    # æ¢ä»¶ç¯©é¸
    server_ids: Optional[List[int]] = None
    collection_success_only: bool = False
    error_records_only: bool = False


class DataCleaner:
    """æ•¸æ“šæ¸…ç†å™¨"""
    
    # é è¨­æ¸…ç†ç­–ç•¥
    DEFAULT_POLICIES = {
        CleanupLevel.BASIC: CleanupPolicy(
            name="åŸºæœ¬æ¸…ç†ï¼ˆ30å¤©ï¼‰",
            retention_days=30,
            archive_before_delete=True,
            batch_size=1000
        ),
        CleanupLevel.AGGRESSIVE: CleanupPolicy(
            name="ç©æ¥µæ¸…ç†ï¼ˆ7å¤©ï¼‰",
            retention_days=7,
            archive_before_delete=False,
            batch_size=2000
        ),
        CleanupLevel.EMERGENCY: CleanupPolicy(
            name="ç·Šæ€¥æ¸…ç†ï¼ˆ1å¤©ï¼‰",
            retention_days=1,
            archive_before_delete=False,
            batch_size=5000,
            error_records_only=True
        )
    }
    
    def __init__(self, archive_path: str = "backend/app/archives"):
        self.archive_path = Path(archive_path)
        self.archive_path.mkdir(parents=True, exist_ok=True)
        self.db_session_factory = get_sync_db
    
    async def cleanup_old_data(
        self,
        cleanup_level: CleanupLevel = CleanupLevel.BASIC,
        custom_policy: Optional[CleanupPolicy] = None
    ) -> CleanupStats:
        """
        æ¸…ç†èˆŠæ•¸æ“šä¸»è¦å…¥å£
        
        Args:
            cleanup_level: æ¸…ç†ç­‰ç´š
            custom_policy: è‡ªå®šç¾©æ¸…ç†ç­–ç•¥
        """
        start_time = time.time()
        stats = CleanupStats()
        
        try:
            # ä½¿ç”¨è‡ªå®šç¾©ç­–ç•¥æˆ–é è¨­ç­–ç•¥
            policy = custom_policy or self.DEFAULT_POLICIES[cleanup_level]
            
            logger.info(f"é–‹å§‹åŸ·è¡Œæ•¸æ“šæ¸…ç†: {policy.name}")
            
            # è¨ˆç®—æ¸…ç†æ™‚é–“é»
            cutoff_date = datetime.now() - timedelta(days=policy.retention_days)
            
            # åŸ·è¡Œæ­¸æª”
            if policy.archive_before_delete:
                archive_stats = await self._archive_old_data(cutoff_date, policy)
                stats.archived_records = archive_stats.archived_records
                stats.archived_size_bytes = archive_stats.archived_size_bytes
                stats.errors.extend(archive_stats.errors)
            
            # åŸ·è¡Œæ¸…ç†
            cleanup_stats = await self._delete_old_data(cutoff_date, policy)
            stats.cleaned_records = cleanup_stats.cleaned_records
            stats.cleaned_size_bytes = cleanup_stats.cleaned_size_bytes
            stats.errors.extend(cleanup_stats.errors)
            
            stats.cleanup_time = time.time() - start_time
            
            logger.info(f"æ•¸æ“šæ¸…ç†å®Œæˆ: æ¸…ç† {stats.cleaned_records} ç­†è¨˜éŒ„, "
                       f"æ­¸æª” {stats.archived_records} ç­†è¨˜éŒ„, è€—æ™‚ {stats.cleanup_time:.2f}s")
            
            return stats
            
        except Exception as e:
            logger.error(f"æ•¸æ“šæ¸…ç†å¤±æ•—: {e}")
            stats.errors.append(str(e))
            stats.cleanup_time = time.time() - start_time
            return stats
    
    async def _archive_old_data(self, cutoff_date: datetime, policy: CleanupPolicy) -> CleanupStats:
        """æ­¸æª”èˆŠæ•¸æ“š"""
        stats = CleanupStats()
        
        try:
            db = self.db_session_factory()
            try:
                # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
                query = db.query(SystemMetrics).filter(
                    SystemMetrics.timestamp < cutoff_date
                )
                
                # æ‡‰ç”¨é¡å¤–éæ¿¾æ¢ä»¶
                if policy.server_ids:
                    query = query.filter(SystemMetrics.server_id.in_(policy.server_ids))
                
                if policy.collection_success_only:
                    query = query.filter(SystemMetrics.collection_success == True)
                elif policy.error_records_only:
                    query = query.filter(SystemMetrics.collection_success == False)
                
                # åˆ†æ‰¹è™•ç†æ­¸æª”
                total_records = query.count()
                if total_records == 0:
                    return stats
                
                logger.info(f"é–‹å§‹æ­¸æª” {total_records} ç­†èˆŠæ•¸æ“šåˆ° {cutoff_date}")
                
                # å‰µå»ºæ­¸æª”ç›®éŒ„
                archive_date = cutoff_date.strftime("%Y%m%d")
                archive_dir = self.archive_path / f"metrics_{archive_date}"
                archive_dir.mkdir(exist_ok=True)
                
                batch_count = 0
                offset = 0
                
                while offset < total_records:
                    # æ‰¹é‡æŸ¥è©¢
                    batch_records = query.offset(offset).limit(policy.batch_size).all()
                    if not batch_records:
                        break
                    
                    # è½‰æ›ç‚º JSON æ ¼å¼
                    archive_data = []
                    for record in batch_records:
                        archive_data.append({
                            "id": record.id,
                            "server_id": record.server_id,
                            "timestamp": record.timestamp.isoformat(),
                            "cpu_usage_percent": record.cpu_usage_percent,
                            "memory_usage_percent": record.memory_usage_percent,
                            "disk_usage_percent": record.disk_usage_percent,
                            "load_average_1m": record.load_average_1m,
                            "memory_total_mb": record.memory_total_mb,
                            "disk_total_gb": record.disk_total_gb,
                            "network_bytes_sent_per_sec": record.network_bytes_sent_per_sec,
                            "network_bytes_recv_per_sec": record.network_bytes_recv_per_sec,
                            "collection_success": record.collection_success,
                            "error_message": record.error_message
                        })
                    
                    # å¯«å…¥æ­¸æª”æª”æ¡ˆ
                    archive_file = archive_dir / f"batch_{batch_count:04d}.json"
                    with open(archive_file, 'w', encoding='utf-8') as f:
                        json.dump(archive_data, f, ensure_ascii=False, indent=2)
                    
                    # è¨ˆç®—æª”æ¡ˆå¤§å°
                    file_size = archive_file.stat().st_size
                    stats.archived_size_bytes += file_size
                    stats.archived_records += len(batch_records)
                    
                    batch_count += 1
                    offset += policy.batch_size
                    
                    # é€²åº¦æ—¥èªŒ
                    if batch_count % 10 == 0:
                        progress = (offset / total_records) * 100
                        logger.info(f"æ­¸æª”é€²åº¦: {progress:.1f}% ({offset}/{total_records})")
                
                # å‰µå»ºæ­¸æª”æ‘˜è¦æª”æ¡ˆ
                summary_file = archive_dir / "archive_summary.json"
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "archive_date": cutoff_date.isoformat(),
                        "total_records": stats.archived_records,
                        "total_size_bytes": stats.archived_size_bytes,
                        "batch_files": batch_count,
                        "policy": {
                            "name": policy.name,
                            "retention_days": policy.retention_days,
                            "batch_size": policy.batch_size
                        },
                        "created_at": datetime.now().isoformat()
                    }, f, ensure_ascii=False, indent=2)
                
                logger.info(f"æ­¸æª”å®Œæˆ: {stats.archived_records} ç­†è¨˜éŒ„, "
                           f"å¤§å° {stats.archived_size_bytes / (1024**2):.2f}MB")
                
                return stats
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"æ­¸æª”å¤±æ•—: {e}")
            stats.errors.append(str(e))
            return stats
    
    async def _delete_old_data(self, cutoff_date: datetime, policy: CleanupPolicy) -> CleanupStats:
        """åˆªé™¤èˆŠæ•¸æ“š"""
        stats = CleanupStats()
        
        try:
            db = self.db_session_factory()
            try:
                # æ§‹å»ºåˆªé™¤æŸ¥è©¢
                delete_query = delete(SystemMetrics).where(
                    SystemMetrics.timestamp < cutoff_date
                )
                
                # æ‡‰ç”¨é¡å¤–éæ¿¾æ¢ä»¶
                if policy.server_ids:
                    delete_query = delete_query.where(
                        SystemMetrics.server_id.in_(policy.server_ids)
                    )
                
                if policy.collection_success_only:
                    delete_query = delete_query.where(
                        SystemMetrics.collection_success == True
                    )
                elif policy.error_records_only:
                    delete_query = delete_query.where(
                        SystemMetrics.collection_success == False
                    )
                
                # å…ˆè¨ˆç®—è¦åˆªé™¤çš„è¨˜éŒ„æ•¸
                count_query = db.query(SystemMetrics).filter(
                    SystemMetrics.timestamp < cutoff_date
                )
                if policy.server_ids:
                    count_query = count_query.filter(
                        SystemMetrics.server_id.in_(policy.server_ids)
                    )
                if policy.collection_success_only:
                    count_query = count_query.filter(
                        SystemMetrics.collection_success == True
                    )
                elif policy.error_records_only:
                    count_query = count_query.filter(
                        SystemMetrics.collection_success == False
                    )
                
                total_to_delete = count_query.count()
                
                if total_to_delete == 0:
                    logger.info("æ²’æœ‰éœ€è¦æ¸…ç†çš„èˆŠæ•¸æ“š")
                    return stats
                
                logger.info(f"é–‹å§‹åˆªé™¤ {total_to_delete} ç­†èˆŠæ•¸æ“š (æ—©æ–¼ {cutoff_date})")
                
                # åŸ·è¡Œåˆªé™¤
                result = db.execute(delete_query)
                stats.cleaned_records = result.rowcount
                
                # ä¼°ç®—æ¸…ç†çš„ç©ºé–“å¤§å° (å‡è¨­æ¯ç­†è¨˜éŒ„ç´„ 1KB)
                stats.cleaned_size_bytes = stats.cleaned_records * 1024
                
                # æäº¤äº‹å‹™
                db.commit()
                
                logger.info(f"æ¸…ç†å®Œæˆ: åˆªé™¤ {stats.cleaned_records} ç­†è¨˜éŒ„")
                
                return stats
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"åˆªé™¤èˆŠæ•¸æ“šå¤±æ•—: {e}")
            stats.errors.append(str(e))
            return stats
    
    async def get_storage_info(self) -> StorageInfo:
        """å–å¾—å„²å­˜ç©ºé–“è³‡è¨Š"""
        try:
            # å–å¾—ç³»çµ±ç£ç¢Ÿä½¿ç”¨æƒ…æ³
            disk_usage = shutil.disk_usage("/")
            
            storage_info = StorageInfo(
                total_space_bytes=disk_usage.total,
                used_space_bytes=disk_usage.used,
                free_space_bytes=disk_usage.free,
                usage_percentage=round((disk_usage.used / disk_usage.total) * 100, 2)
            )
            
            # è¨ˆç®—è³‡æ–™åº«å¤§å°
            try:
                db = self.db_session_factory()
                try:
                    # PostgreSQL æŸ¥è©¢è³‡æ–™åº«å¤§å°
                    result = db.execute(text(
                        "SELECT pg_database_size(current_database()) as db_size"
                    )).fetchone()
                    if result:
                        storage_info.database_size_bytes = result.db_size
                finally:
                    db.close()
            except Exception as e:
                logger.warning(f"å–å¾—è³‡æ–™åº«å¤§å°å¤±æ•—: {e}")
            
            # è¨ˆç®—æ­¸æª”ç›®éŒ„å¤§å°
            try:
                archive_size = sum(
                    f.stat().st_size 
                    for f in self.archive_path.rglob('*') 
                    if f.is_file()
                )
                storage_info.archive_size_bytes = archive_size
            except Exception as e:
                logger.warning(f"å–å¾—æ­¸æª”å¤§å°å¤±æ•—: {e}")
            
            return storage_info
            
        except Exception as e:
            logger.error(f"å–å¾—å„²å­˜ç©ºé–“è³‡è¨Šå¤±æ•—: {e}")
            return StorageInfo(
                total_space_bytes=0,
                used_space_bytes=0,
                free_space_bytes=0,
                usage_percentage=0.0
            )
    
    async def cleanup_archive_files(self, days_to_keep: int = 90) -> CleanupStats:
        """æ¸…ç†èˆŠçš„æ­¸æª”æª”æ¡ˆ"""
        stats = CleanupStats()
        start_time = time.time()
        
        try:
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            
            # æƒææ­¸æª”ç›®éŒ„
            for archive_dir in self.archive_path.iterdir():
                if not archive_dir.is_dir():
                    continue
                
                try:
                    # è§£æç›®éŒ„åç¨±ä¸­çš„æ—¥æœŸ
                    if archive_dir.name.startswith("metrics_"):
                        date_str = archive_dir.name.replace("metrics_", "")
                        archive_date = datetime.strptime(date_str, "%Y%m%d")
                        
                        if archive_date < cutoff_date:
                            # è¨ˆç®—ç›®éŒ„å¤§å°
                            dir_size = sum(
                                f.stat().st_size 
                                for f in archive_dir.rglob('*') 
                                if f.is_file()
                            )
                            
                            # åˆªé™¤ç›®éŒ„
                            shutil.rmtree(archive_dir)
                            
                            stats.cleaned_records += 1
                            stats.cleaned_size_bytes += dir_size
                            
                            logger.info(f"æ¸…ç†æ­¸æª”ç›®éŒ„: {archive_dir.name}")
                            
                except Exception as e:
                    logger.warning(f"æ¸…ç†æ­¸æª”ç›®éŒ„ {archive_dir.name} å¤±æ•—: {e}")
                    stats.errors.append(f"æ¸…ç† {archive_dir.name} å¤±æ•—: {e}")
            
            stats.cleanup_time = time.time() - start_time
            
            logger.info(f"æ­¸æª”æª”æ¡ˆæ¸…ç†å®Œæˆ: æ¸…ç† {stats.cleaned_records} å€‹ç›®éŒ„, "
                       f"é‡‹æ”¾ {stats.cleaned_size_bytes / (1024**2):.2f}MB")
            
            return stats
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ­¸æª”æª”æ¡ˆå¤±æ•—: {e}")
            stats.errors.append(str(e))
            stats.cleanup_time = time.time() - start_time
            return stats
    
    async def get_cleanup_recommendations(self) -> Dict[str, Any]:
        """å–å¾—æ¸…ç†å»ºè­°"""
        try:
            storage_info = await self.get_storage_info()
            
            recommendations = []
            
            # å„²å­˜ç©ºé–“æª¢æŸ¥
            if storage_info.usage_percentage > 90:
                recommendations.append({
                    "level": "critical",
                    "type": "storage_space",
                    "message": "ç£ç¢Ÿä½¿ç”¨ç‡è¶…é 90%ï¼Œå»ºè­°ç«‹å³åŸ·è¡Œç·Šæ€¥æ¸…ç†",
                    "action": "emergency_cleanup",
                    "priority": 1
                })
            elif storage_info.usage_percentage > 80:
                recommendations.append({
                    "level": "warning",
                    "type": "storage_space",
                    "message": "ç£ç¢Ÿä½¿ç”¨ç‡è¶…é 80%ï¼Œå»ºè­°åŸ·è¡Œç©æ¥µæ¸…ç†",
                    "action": "aggressive_cleanup",
                    "priority": 2
                })
            
            # è³‡æ–™åº«å¤§å°æª¢æŸ¥
            db_size_gb = storage_info.database_size_bytes / (1024**3)
            if db_size_gb > 10:
                recommendations.append({
                    "level": "info",
                    "type": "database_size",
                    "message": f"è³‡æ–™åº«å¤§å° {db_size_gb:.1f}GBï¼Œå»ºè­°å®šæœŸæ¸…ç†èˆŠæ•¸æ“š",
                    "action": "basic_cleanup",
                    "priority": 3
                })
            
            # æ­¸æª”å¤§å°æª¢æŸ¥
            archive_size_gb = storage_info.archive_size_bytes / (1024**3)
            if archive_size_gb > 5:
                recommendations.append({
                    "level": "info",
                    "type": "archive_size",
                    "message": f"æ­¸æª”å¤§å° {archive_size_gb:.1f}GBï¼Œå»ºè­°æ¸…ç†èˆŠæ­¸æª”æª”æ¡ˆ",
                    "action": "cleanup_archives",
                    "priority": 4
                })
            
            # æª¢æŸ¥èˆŠæ•¸æ“šæ•¸é‡
            try:
                db = self.db_session_factory()
                try:
                    # æª¢æŸ¥30å¤©ä»¥ä¸Šçš„æ•¸æ“š
                    old_data_count = db.query(SystemMetrics).filter(
                        SystemMetrics.timestamp < datetime.now() - timedelta(days=30)
                    ).count()
                    
                    if old_data_count > 100000:  # è¶…é10è¬ç­†
                        recommendations.append({
                            "level": "warning",
                            "type": "old_data",
                            "message": f"ç™¼ç¾ {old_data_count} ç­†30å¤©ä»¥ä¸Šçš„èˆŠæ•¸æ“šï¼Œå»ºè­°åŸ·è¡Œæ¸…ç†",
                            "action": "basic_cleanup",
                            "priority": 2
                        })
                finally:
                    db.close()
            except Exception as e:
                logger.warning(f"æª¢æŸ¥èˆŠæ•¸æ“šæ•¸é‡å¤±æ•—: {e}")
            
            # æ’åºå»ºè­°ï¼ˆæŒ‰å„ªå…ˆç´šï¼‰
            recommendations.sort(key=lambda x: x["priority"])
            
            return {
                "storage_info": storage_info.to_dict(),
                "recommendations": recommendations,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"å–å¾—æ¸…ç†å»ºè­°å¤±æ•—: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }


class ScheduledCleaner:
    """å®šæ™‚æ¸…ç†å™¨"""
    
    def __init__(self, cleaner: DataCleaner):
        self.cleaner = cleaner
        self.is_running = False
    
    async def start_scheduled_cleanup(
        self,
        cleanup_interval_hours: int = 24,
        cleanup_level: CleanupLevel = CleanupLevel.BASIC
    ):
        """é–‹å§‹å®šæ™‚æ¸…ç†"""
        if self.is_running:
            logger.warning("å®šæ™‚æ¸…ç†å·²åœ¨é‹è¡Œä¸­")
            return
        
        self.is_running = True
        logger.info(f"é–‹å§‹å®šæ™‚æ¸…ç†ï¼Œé–“éš” {cleanup_interval_hours} å°æ™‚")
        
        try:
            while self.is_running:
                # åŸ·è¡Œæ¸…ç†
                stats = await self.cleaner.cleanup_old_data(cleanup_level)
                
                if stats.errors:
                    logger.error(f"å®šæ™‚æ¸…ç†å‡ºç¾éŒ¯èª¤: {stats.errors}")
                else:
                    logger.info(f"å®šæ™‚æ¸…ç†å®Œæˆ: {stats.to_dict()}")
                
                # ç­‰å¾…ä¸‹æ¬¡æ¸…ç†
                await asyncio.sleep(cleanup_interval_hours * 3600)
                
        except asyncio.CancelledError:
            logger.info("å®šæ™‚æ¸…ç†è¢«å–æ¶ˆ")
        except Exception as e:
            logger.error(f"å®šæ™‚æ¸…ç†ç™¼ç”Ÿç•°å¸¸: {e}")
        finally:
            self.is_running = False
    
    def stop_scheduled_cleanup(self):
        """åœæ­¢å®šæ™‚æ¸…ç†"""
        self.is_running = False
        logger.info("åœæ­¢å®šæ™‚æ¸…ç†")


# å…¨åŸŸå¯¦ä¾‹
data_cleaner = DataCleaner()
scheduled_cleaner = ScheduledCleaner(data_cleaner)


# ä¾¿åˆ©å‡½æ•¸
async def cleanup_old_monitoring_data(
    days_to_keep: int = 30,
    archive_before_delete: bool = True
) -> CleanupStats:
    """æ¸…ç†èˆŠç›£æ§æ•¸æ“šçš„ä¾¿åˆ©å‡½æ•¸"""
    policy = CleanupPolicy(
        name=f"è‡ªå®šç¾©æ¸…ç†ï¼ˆ{days_to_keep}å¤©ï¼‰",
        retention_days=days_to_keep,
        archive_before_delete=archive_before_delete
    )
    return await data_cleaner.cleanup_old_data(custom_policy=policy)


async def get_storage_status() -> StorageInfo:
    """å–å¾—å„²å­˜ç‹€æ…‹çš„ä¾¿åˆ©å‡½æ•¸"""
    return await data_cleaner.get_storage_info()


async def get_cleanup_suggestions() -> Dict[str, Any]:
    """å–å¾—æ¸…ç†å»ºè­°çš„ä¾¿åˆ©å‡½æ•¸"""
    return await data_cleaner.get_cleanup_recommendations()


if __name__ == "__main__":
    # æ¸¬è©¦æ•¸æ“šæ¸…ç†å™¨
    
    async def test_storage_info():
        """æ¸¬è©¦å„²å­˜ç©ºé–“è³‡è¨Š"""
        print("ğŸ’¾ æ¸¬è©¦å„²å­˜ç©ºé–“è³‡è¨Š...")
        
        try:
            storage_info = await data_cleaner.get_storage_info()
            
            print(f"âœ… å„²å­˜ç©ºé–“è³‡è¨Š:")
            print(f"  - ç¸½ç©ºé–“: {storage_info.total_space_bytes / (1024**3):.2f}GB")
            print(f"  - å·²ç”¨ç©ºé–“: {storage_info.used_space_bytes / (1024**3):.2f}GB")
            print(f"  - å¯ç”¨ç©ºé–“: {storage_info.free_space_bytes / (1024**3):.2f}GB")
            print(f"  - ä½¿ç”¨ç‡: {storage_info.usage_percentage}%")
            print(f"  - è³‡æ–™åº«å¤§å°: {storage_info.database_size_bytes / (1024**2):.2f}MB")
            
        except Exception as e:
            print(f"âŒ å„²å­˜ç©ºé–“è³‡è¨Šæ¸¬è©¦å¤±æ•—: {e}")
    
    async def test_cleanup_recommendations():
        """æ¸¬è©¦æ¸…ç†å»ºè­°"""
        print("\nğŸ“‹ æ¸¬è©¦æ¸…ç†å»ºè­°...")
        
        try:
            recommendations = await data_cleaner.get_cleanup_recommendations()
            
            print(f"âœ… æ¸…ç†å»ºè­°:")
            
            if "recommendations" in recommendations:
                for i, rec in enumerate(recommendations["recommendations"], 1):
                    print(f"  {i}. [{rec['level'].upper()}] {rec['message']}")
                    print(f"     å»ºè­°å‹•ä½œ: {rec['action']}")
            else:
                print("  ç›®å‰ç„¡æ¸…ç†å»ºè­°")
                
        except Exception as e:
            print(f"âŒ æ¸…ç†å»ºè­°æ¸¬è©¦å¤±æ•—: {e}")
    
    async def test_data_cleanup():
        """æ¸¬è©¦æ•¸æ“šæ¸…ç†"""
        print("\nğŸ§¹ æ¸¬è©¦æ•¸æ“šæ¸…ç†...")
        
        try:
            # å‰µå»ºæ¸¬è©¦æ¸…ç†ç­–ç•¥
            test_policy = CleanupPolicy(
                name="æ¸¬è©¦æ¸…ç†",
                retention_days=365,  # ä¿ç•™1å¹´ä»¥ä¸Šçš„æ•¸æ“šé€²è¡Œæ¸¬è©¦
                archive_before_delete=True,
                batch_size=100
            )
            
            stats = await data_cleaner.cleanup_old_data(custom_policy=test_policy)
            
            print(f"âœ… æ•¸æ“šæ¸…ç†å®Œæˆ:")
            print(f"  - æ¸…ç†è¨˜éŒ„: {stats.cleaned_records} ç­†")
            print(f"  - æ­¸æª”è¨˜éŒ„: {stats.archived_records} ç­†")
            print(f"  - è™•ç†æ™‚é–“: {stats.cleanup_time:.2f}s")
            
            if stats.errors:
                print(f"  - éŒ¯èª¤: {stats.errors}")
                
        except Exception as e:
            print(f"âŒ æ•¸æ“šæ¸…ç†æ¸¬è©¦å¤±æ•—: {e}")
    
    async def test_complete():
        """å®Œæ•´æ¸¬è©¦"""
        print("=" * 50)
        print("ğŸ§ª CWatcher æ•¸æ“šæ¸…ç†èˆ‡æ­¸æª”æœå‹™æ¸¬è©¦")
        print("=" * 50)
        
        await test_storage_info()
        await test_cleanup_recommendations()
        await test_data_cleanup()
        
        print("\nâœ… æ•¸æ“šæ¸…ç†èˆ‡æ­¸æª”æœå‹™æ¸¬è©¦å®Œæˆ")
    
    # åŸ·è¡Œæ¸¬è©¦
    import asyncio
    asyncio.run(test_complete())